{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "from contextlib import suppress\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from fnmatch import fnmatch\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "\n",
    "global graph,model\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "import pympi\n",
    "\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "from common import estimate_pose, draw_humans, read_imgfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for folders\n",
    "\n",
    "directory1 = \"./Input/orig_frames\"\n",
    "if not os.path.exists(directory1):\n",
    "    os.makedirs(directory1)\n",
    "\n",
    "directory2 = \"./Outputs/openpos\"\n",
    "if not os.path.exists(directory2):\n",
    "    os.makedirs(directory2)\n",
    "    \n",
    "directory3 = \"./Outputs/final\"\n",
    "if not os.path.exists(directory3):\n",
    "    os.makedirs(directory3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(imgpath, j):\n",
    "    #input_width = 656\n",
    "    #input_height = 368\n",
    "    \n",
    "    input_width = 352\n",
    "    input_height = 288\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    from tensorflow.core.framework import graph_pb2\n",
    "    graph_def = graph_pb2.GraphDef()\n",
    "    # Download model from https://www.dropbox.com/s/2dw1oz9l9hi9avg/optimized_openpose.pb\n",
    "    with open('models/optimized_openpose.pb', 'rb') as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    t1 = time.time()\n",
    "    #print(t1 - t0)\n",
    "\n",
    "    inputs = tf.get_default_graph().get_tensor_by_name('inputs:0')\n",
    "    heatmaps_tensor = tf.get_default_graph().get_tensor_by_name('Mconv7_stage6_L2/BiasAdd:0')\n",
    "    pafs_tensor = tf.get_default_graph().get_tensor_by_name('Mconv7_stage6_L1/BiasAdd:0')\n",
    "\n",
    "    t2 = time.time()\n",
    "    #print(t2 - t1)\n",
    "\n",
    "    image = read_imgfile(imgpath, input_width, input_height)\n",
    "\n",
    "    t3 = time.time()\n",
    "    #print(t3 - t2)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        heatMat, pafMat = sess.run([heatmaps_tensor, pafs_tensor], feed_dict={\n",
    "            inputs: image\n",
    "        })\n",
    "\n",
    "        t4 = time.time()\n",
    "        #print(t4 - t3)\n",
    "\n",
    "        heatMat, pafMat = heatMat[0], pafMat[0]\n",
    "\n",
    "        humans = estimate_pose(heatMat, pafMat)\n",
    "        \n",
    "        #coordinates for left hand, head, right hand etc. indexes: 0,2,3,4,5,6,7\n",
    "        head_coco_idx = 0\n",
    "        right_shoulder_idx = 2\n",
    "        right_elbow_idx = 3\n",
    "        right_hand_idx = 4\n",
    "        left_shoulder_idx = 5\n",
    "        left_elbow_idx = 6\n",
    "        left_hand_idx = 7\n",
    "\n",
    "        for human in humans:\n",
    "            if human[right_hand_idx] is None:\n",
    "                break\n",
    "                \n",
    "            head_coords = human[head_coco_idx][1]\n",
    "            right_shoulder_coords = human[right_shoulder_idx][1]\n",
    "            right_elbow_coords = human[right_elbow_idx][1]\n",
    "            right_hand_coords = human[right_hand_idx][1]\n",
    "            left_shoulder_coords = human[left_shoulder_idx][1]\n",
    "            left_elbow_coords = human[left_elbow_idx][1]\n",
    "            left_hand_coords = human[left_hand_idx][1]\n",
    "            \n",
    "            fields = [head_coords, right_shoulder_coords, right_elbow_coords,right_hand_coords,  left_shoulder_coords, left_elbow_coords, left_hand_coords]\n",
    "            \n",
    "        \n",
    "        #with open(r'test.csv', 'a') as f:\n",
    "            #writer = csv.writer(f)\n",
    "            #writer.writerow(fields)\n",
    "        # end of printing\n",
    "        \n",
    "        # display\n",
    "        image = cv2.imread(imgpath)\n",
    "        image_h, image_w = image.shape[:2]\n",
    "        image = draw_humans(image, humans)\n",
    "\n",
    "        scale = 480.0 / image_h\n",
    "        newh, neww = 480, int(scale * image_w + 0.5)\n",
    "\n",
    "        image = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "        ### Uncomment below to show image with skeleton\n",
    "        #cv2.imshow('result', image)\n",
    "        \n",
    "        ##### save image with coordinates\n",
    "        cv2.imwrite('./Outputs/openpos/result_'+str(j)+'.png',image)\n",
    "        \n",
    "        t5 = time.time()\n",
    "        #print(t5 - t4)\n",
    "        cv2.waitKey(0)\n",
    "        \n",
    "        return fields\n",
    "\t\t\n",
    "def video_to_frames(input_loc, output_loc):\n",
    "\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) -1\n",
    "    print(video_length)\n",
    "    print (\"Number of frames: \", video_length)\n",
    "    count = 0\n",
    "    print (\"Converting video..\\n\")\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        # Write the results back to output location.\n",
    "        resized_frame = cv2.resize(frame, (352, 288), interpolation=cv2.INTER_AREA)\n",
    "        cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count+1), resized_frame)\n",
    "        count = count + 1\n",
    "        # If there are no more frames left\n",
    "        if (count > (video_length-1)):\n",
    "            # Log the time again\n",
    "            time_end = time.time()\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print (\"Done extracting frames.\\n%d frames extracted\" % count)\n",
    "            print (\"It took %d seconds forconversion.\" % (time_end-time_start) + str(\"\\n\\n\"))\n",
    "            break\n",
    "            \n",
    "    return(video_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "Number of frames:  174\n",
      "Converting video..\n",
      "\n",
      "Done extracting frames.\n",
      "174 frames extracted\n",
      "It took 1 seconds forconversion.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract frames to Input/orig_frames/ folder\n",
    "\n",
    "total_frames = video_to_frames(\"african_video.mp4\", \"./Input/orig_frames/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xgbooster model\n",
    "loaded_model = pickle.load(open(\"xgboost_model.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load keras model (optional)\n",
    "model = load_model('multilayer_perceptron_7_rmsprop_300.h5')\n",
    "\n",
    "def ker_predictions(points2):\n",
    "    with graph.as_default():    \n",
    "        preds = model.predict_classes(points2)\n",
    "        #print(\"predicted frame: \"+ str(preds))                \n",
    "\n",
    "        if preds==0:\n",
    "            overlay = 'Not signing'\n",
    "            predictions_array.append(0)\n",
    "        else:\n",
    "            overlay = 'Signing'\n",
    "            predictions_array.append(1)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting\n",
      "predicting frame: ./Input/orig_frames/00001.jpg\n"
     ]
    }
   ],
   "source": [
    "# Run Predictions using XGBoost\n",
    "\n",
    "root2 = './Input/orig_frames/'\n",
    "pattern = \"*.jpg\"\n",
    "total_files = 0\n",
    "total_scanned_files = 0\n",
    "\n",
    "predictions_array = []\n",
    "\n",
    "overlay = 'Signing'\n",
    "print(\"Start predicting\")\n",
    "for root2, dirs, files in os.walk(root2, topdown=False):\n",
    "    for name in files:\n",
    "        #total_files = total_files + 1\n",
    "        if fnmatch(name, pattern):\n",
    "            #print(os.path.join(root2, name))\n",
    "            #Running openpose  \n",
    "            #with suppress(Exception):\n",
    "            j = re.sub('\\.jpg$', '', name)\n",
    "            fpath = str(os.path.join(root2, name))\n",
    "            print(\"predicting frame: \"+str(fpath))\n",
    "            #    try:\n",
    "                    #print('predicting frame: '+str(fname))\n",
    "            coord = inference(fpath, j)\n",
    "             #   except TypeError:\n",
    "                    #print(\"Not a person in frame: \"+str(fname))\n",
    "                    #coord = np.zeros((7, 2))\n",
    "              #  except:\n",
    "                    #print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                    #print(\"not file found in frame: \"+str(fname))\n",
    "                    #coord = np.zeros((7, 2))            \n",
    "            coord2 = np.asarray(coord).flatten()\n",
    "            points = coord2.reshape(1,14)\n",
    "            points2 = np.float64(points)\n",
    "            # keras prediction (optional)\n",
    "            #ker_predictions(points2)\n",
    "            \n",
    "            # make predictions for data\n",
    "            y_pred = loaded_model.predict(points2)\n",
    "            preds = [round(value) for value in y_pred]\n",
    "            \n",
    "            if preds==0:\n",
    "                overlay = 'Not signing'\n",
    "                predictions_array.append(0)\n",
    "            else:\n",
    "                overlay = 'Signing'\n",
    "                predictions_array.append(1)  \n",
    "\n",
    "                #font = ImageFont.truetype(\"Arial.ttf\", 25)\n",
    "                #img = Image.open('./Outputs/openpos/result_'+str(j)+'.png')\n",
    "                #draw = ImageDraw.Draw(img)\n",
    "                #draw.text((20,20), str(overlay), (255,255,0), font=font)\n",
    "                #draw = ImageDraw.Draw(img)\n",
    "                #img.save('./Outputs/final/'+str(j)+'.png')                   \n",
    "                #os.remove('./Outputs/openpos/result_'+str(j)+'.png')\n",
    "\n",
    "print(\"done predicting\")\n",
    "len(predictions_array)\n",
    "q = np.asarray(predictions_array)\n",
    "np.savetxt('pred.txt', q , delimiter=',')\n",
    "print(\"saved predictions in txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create video from extracted frames with predictions\n",
    "\n",
    "dir_path = './Outputs/final/'\n",
    "ext = 'png'\n",
    "output = 'output_video.mp4'\n",
    "\n",
    "images = []\n",
    "for f in os.listdir(dir_path):\n",
    "    if f.endswith(ext):\n",
    "        images.append(f)\n",
    "\n",
    "# Determine the width and height from the first image\n",
    "image_path = os.path.join(dir_path, images[0])\n",
    "frame = cv2.imread(image_path)\n",
    "cv2.imshow('video',frame)\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Be sure to use lower case\n",
    "out = cv2.VideoWriter(output, fourcc, 20.0, (width, height))\n",
    "\n",
    "for image in images:\n",
    "\n",
    "    image_path = os.path.join(dir_path, image)\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    out.write(frame) # Write out frame to video\n",
    "\n",
    "    cv2.imshow('video',frame)\n",
    "    if (cv2.waitKey(1) & 0xFF) == ord('q'): # Hit `q` to exit\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"The output video is {}\".format(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotations based on predictions array\n",
    "\n",
    "x = pympi.Eaf(author=\"Manolis\")\n",
    "x.add_tier(tier_id=\"tier1\")\n",
    "p = np.asarray(predictions_array)\n",
    "s = np.asarray(p)\n",
    "size = s.size\n",
    "#print(size)\n",
    "#print(size)\n",
    "st = 0\n",
    "end = 0\n",
    "arr = []\n",
    "j = 0\n",
    "i = 0\n",
    "\n",
    "#master_boolean = True\n",
    "#print(\"strarting values: \" + str(st))\n",
    "while i< size-1:     \n",
    "    if s[i] == 1:\n",
    "        j = i + 1\n",
    "        while s[j] == 1 and j<size-1:\n",
    "            end = j\n",
    "            j +=1\n",
    "        print(\"the i: \"+str(i)+\" the j: \"+str(j))\n",
    "        x.add_annotation(id_tier=\"tier1\", start=i*50, end= j*50, value='signing')\n",
    "        i = j\n",
    "        #else:            \n",
    "            #i = j\n",
    "            \n",
    "            #break    \n",
    "    else:\n",
    "        i+=1\n",
    "\n",
    "x.add_linked_file(file_path='testing.mp4',relpath='testing.mp4',mimetype='mp4')\n",
    "pympi.Elan.to_eaf(file_path=\"testing.eaf\", eaf_obj=x, pretty=True)\n",
    "print(\"Elan file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Delete folders with pictures\n",
    "\n",
    "shutil.rmtree('/Input/orig_frames/', ignore_errors=True)\n",
    "shutil.rmtree('/Outputs/openpos', ignore_errors=True)\n",
    "print(\"folders deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
